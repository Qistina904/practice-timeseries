{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modl Creation\n",
    "- multiple LR\n",
    "- F stats\n",
    "- split data test n train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression & OLS Method\n",
    "- least square approach\n",
    "- eg: house prediction \n",
    "- estimate effect of each variable\n",
    "- y + b0 + b1X\n",
    "=> b0 = intercept\n",
    "=> b1 = slope \n",
    "=> X = variable -> no of room \n",
    "=> n = no of obs. \n",
    "like one method for measuring this closeness of line is called => least square method\n",
    "=> ie => residuals. yhat - yhat \n",
    "- the distance of that point from the line => residual. at some point this residual may be positive, at some point it may be negative. \n",
    "- So when we are talking about the total residual of the sample, we cannot straightaway just sum them up because some are positive, some are negative. Therefore, we'll define a new quantity called as the residual sum of squares\n",
    "- Now, since this RSS is summing the square of each residual, it is representing the total error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing accuracy of predicted coefficient\n",
    "- we want to use the coefficient of sample regression line as an estimate for the population regression line, \n",
    "- beta_0 how far off will the sample estimate\n",
    "- Beta_1 cap will be from the population coefficients \n",
    "- Sigma square is a variance of the population residues. \n",
    "- Residual is a value which is a difference of actual y from the estimated y.\n",
    "- Since the Sigma square is a variance of these values for all the data points of population, and since population regression line is not known, then the Sigma squared is also not known, we need to estimate it. \n",
    "- We have RSE that is equal to the residual standard error. RSS is the residual sum of squares. \n",
    "- what is practical takeaway from these standard error calculations? to give us our confidence interval,\n",
    "- there is a 95% chance that true value of Beta_1 lies in the interval Beta_1 cap minus two times standard error of Beta_1 or second Beta_1 cap plus two times standard error of Beta_1. It would be any of these. Within this interval, lower value in this and higher value in this. \n",
    "-  We have the 95 percentage confidence that the actual Beta_1 lies in this interval. Similarly, for Beta_0, the 95% confidence interval will be the estimated Beta_0-2*, the standard error of Beta_0 and the higher value will be Beta_0+2* the standard error of Beta_0. \n",
    "- To summarize what we have done is we have two lines. One line we got from sample regression that we did. Other one is hypothetical line which is true line between the population of all points. We wanted to show whether we can approximate the sample line as a population line for that we found out a confidence interval within which the population coefficient will lie. So the Beta_0 of the population line will lie between these two values. We get from the sample regression and the slope of the population regression will lie between the two values of sample regression. We have assigned a probability of how the confident. We know we are saying that there is a 95% chance that population regression coefficient will lie within these intervals. We'll find out using the sample line. Now, another use of the standard error is to establish whether x and y actually have a relationship or not. In a linear model, there is a relationship that is given by Beta_1. We are saying that y is Beta_1*X+ constant. \n",
    "- If you know Beta_1 is zero, it means that there is no relationship. If there is no relationship, then the variable x cannot be predicted. We need to show probability of Beta_1 = 0, being zero is negligible.\n",
    "- Concept Beta_1 has some value that is most probable value . Now if we want to say that we are sufficiently confident that it cannot be zero and it has two parts to it. First, most probable value should be far from zero, and second, standard error of Beta, which is giving us the interval in which Beta_2 lies to be a smaller value. \n",
    "- Basically, we want zero lie in this whole interval in which Beta lies. Let's do it in a proper way, and the method over here that is called this hypothesis testing. We can construct two hypotheses over here. One is H0, no relationship between X and Y, and we have the next thing, that is H alternative, which is written as Ha, that is there is a relationship between x and y. H0 is Beta_1 equal to zero, Ha is a Beta_1 is not equal to zero. We want to basically disapprove the H0. \n",
    "\n",
    "\n",
    "- how to disapprove H0 by calculating something known as t Statistics\n",
    "- if you have t-value, you can get the probability of observing any value equal to absolute or larger.\n",
    "-  small value of p, which will mean that it is highly unlikely that there is no relationship between the predictor and the response. Or which means that we can reject the null hypothesis and declare that there is relationship between x and y. Typically, we use the value like 5% or 1% as a cutoff value for p. That is if p is less than 0.01, then the variable x is significantly impacting y. \n",
    "-  this interval is giving you that the estimated beta that you have can be used to make the statement that the true regression coefficient will lie in the interval given by standard error.\n",
    "- this probability value, if the value is very small-> there is no relationship, which means that there is some relationship. And therefore, we are confident that the room number variable is impacting the house price.\n",
    "-  we have beta 1 and beta 0 value, that is calculated for sample, we calculated the standard error, which is helping us determining two things. One is what is the range in which true value of beta 1 and beta 0 will lie. And second thing is whether there is an actual relationship between this predictor and the response variable. \n",
    "- So to establish that there is a relationship, we calculated t value using the two things. Using t value, we calculated p value, and if this p value is less than the threshold of 1% or 5%, whichever you like to use, then we can say that there is a relationship. So for our model, there is a relationship of house pricing with room number and that relationship is beta 1, which is 9.0997.\n",
    "-  The confidence interval provides an estimate of the range in which the true population coefficient is likely to fall, given the sample data.\n",
    "\n",
    "\n",
    "RSE and R square\n",
    "- r/s between x and Y \n",
    "- we want to know how well does the predicted Y value with actual Y values\n",
    "- RSE represents the standard deviation of the differences between the observed and predicted values (residuals), indicating how spread out these residuals are.\n",
    "- RSE => the average amount that the response will deviate from the true regression line\n",
    "- RSE can also be considered as a measure of lack of fit to this \n",
    "- RSE provide an absolute measure of lack of fit, but since it is measured in units of Y, it is not always clear what constitutes a good RSE, so R-Square provides us with an alternative.\n",
    "- R-Square is a proportion, the proportion of total variance explained by the model, it always lie between zero and one\n",
    "- R-Square measures the proportion of explained variance from the total variance.\n",
    "- Adjusted R-Square will also be taking into account the total number of variables, which are actually impacting the model. The reason behind doing this is, if you keep on adding variables to your model, R-Square value simply keeps on increasing even if the variable is not significantly related with the response variable. Still the R-Square value will increase by a small amount. So the adjusted R-Square is a modified version of this that has been adjusted for a number of predictors in the particular model. The adjusted R-Square increases only if new term improves the model more than what you have expected by chance. It decreases when the predictor improves the model by less than expected by chance, \n",
    "- So this will generally depend on type of application that you have, if data is coming from science experiment, and the relationship is supposed to be actually linear, in such a case, R-Square should be very close to one. But if it is a marketing data, let's say and we are missing a lot of unmeasured factors, and the linear assumption is also a rough approximation of the relationship, the residual errors are going to be large. So in such a case, even smaller R-Square values can be acceptable. Generally, R-Square greater than 0.5 can be considered as a good fit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Regression Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = pd.read_csv('house price.csv', header=0, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>Exterior1st</th>\n",
       "      <th>Exterior2nd</th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>Foundation</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>Heating</th>\n",
       "      <th>HeatingQC</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>Electrical</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Functional</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>196.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>706</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>8</td>\n",
       "      <td>Typ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>548</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>978</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>162.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Mn</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>486</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Wd Sdng</td>\n",
       "      <td>Wd Shng</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>BrkTil</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>No</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>216</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>7</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>3</td>\n",
       "      <td>642</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>BrkFace</td>\n",
       "      <td>350.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>Av</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>655</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>9</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>3</td>\n",
       "      <td>836</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Gilbert</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>PConc</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>953</td>\n",
       "      <td>953</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>953</td>\n",
       "      <td>694</td>\n",
       "      <td>0</td>\n",
       "      <td>1647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>7</td>\n",
       "      <td>Typ</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NWAmes</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Plywood</td>\n",
       "      <td>Plywood</td>\n",
       "      <td>Stone</td>\n",
       "      <td>119.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>ALQ</td>\n",
       "      <td>790</td>\n",
       "      <td>Rec</td>\n",
       "      <td>163</td>\n",
       "      <td>589</td>\n",
       "      <td>1542</td>\n",
       "      <td>GasA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2073</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>7</td>\n",
       "      <td>Min1</td>\n",
       "      <td>2</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>CemntBd</td>\n",
       "      <td>CmentBd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Stone</td>\n",
       "      <td>TA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>No</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>275</td>\n",
       "      <td>Unf</td>\n",
       "      <td>0</td>\n",
       "      <td>877</td>\n",
       "      <td>1152</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Ex</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1188</td>\n",
       "      <td>1152</td>\n",
       "      <td>0</td>\n",
       "      <td>2340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>9</td>\n",
       "      <td>Typ</td>\n",
       "      <td>2</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>1</td>\n",
       "      <td>252</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GdPrv</td>\n",
       "      <td>Shed</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NAmes</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>Hip</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Mn</td>\n",
       "      <td>GLQ</td>\n",
       "      <td>49</td>\n",
       "      <td>Rec</td>\n",
       "      <td>1029</td>\n",
       "      <td>0</td>\n",
       "      <td>1078</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Y</td>\n",
       "      <td>FuseA</td>\n",
       "      <td>1078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>5</td>\n",
       "      <td>Typ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Edwards</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>HdBoard</td>\n",
       "      <td>HdBoard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Gd</td>\n",
       "      <td>TA</td>\n",
       "      <td>CBlock</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>No</td>\n",
       "      <td>BLQ</td>\n",
       "      <td>830</td>\n",
       "      <td>LwQ</td>\n",
       "      <td>290</td>\n",
       "      <td>136</td>\n",
       "      <td>1256</td>\n",
       "      <td>GasA</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Y</td>\n",
       "      <td>SBrkr</td>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1256</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>6</td>\n",
       "      <td>Typ</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>Fin</td>\n",
       "      <td>1</td>\n",
       "      <td>276</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>736</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass MSZoning  LotFrontage  ...  SaleType SaleCondition SalePrice\n",
       "Id                                      ...                                  \n",
       "1             60       RL         65.0  ...        WD        Normal    208500\n",
       "2             20       RL         80.0  ...        WD        Normal    181500\n",
       "3             60       RL         68.0  ...        WD        Normal    223500\n",
       "4             70       RL         60.0  ...        WD       Abnorml    140000\n",
       "5             60       RL         84.0  ...        WD        Normal    250000\n",
       "...          ...      ...          ...  ...       ...           ...       ...\n",
       "1456          60       RL         62.0  ...        WD        Normal    175000\n",
       "1457          20       RL         85.0  ...        WD        Normal    210000\n",
       "1458          70       RL         66.0  ...        WD        Normal    266500\n",
       "1459          20       RL         68.0  ...        WD        Normal    142125\n",
       "1460          20       RL         75.0  ...        WD        Normal    147500\n",
       "\n",
       "[1460 rows x 80 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = hp[['SalePrice', 'LotArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>LotArea</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>208500</td>\n",
       "      <td>8450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181500</td>\n",
       "      <td>9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>223500</td>\n",
       "      <td>11250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140000</td>\n",
       "      <td>9550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250000</td>\n",
       "      <td>14260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>175000</td>\n",
       "      <td>7917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>210000</td>\n",
       "      <td>13175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>266500</td>\n",
       "      <td>9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>142125</td>\n",
       "      <td>9717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>147500</td>\n",
       "      <td>9937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SalePrice  LotArea\n",
       "Id                      \n",
       "1        208500     8450\n",
       "2        181500     9600\n",
       "3        223500    11250\n",
       "4        140000     9550\n",
       "5        250000    14260\n",
       "...         ...      ...\n",
       "1456     175000     7917\n",
       "1457     210000    13175\n",
       "1458     266500     9042\n",
       "1459     142125     9717\n",
       "1460     147500     9937\n",
       "\n",
       "[1460 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvar = sn.add_constant(hp['LotArea'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmvar = sn.OLS(hp['SalePrice'], Xvar).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>SalePrice</td>    <th>  R-squared:         </th> <td>   0.070</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.069</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   109.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 25 Dec 2023</td> <th>  Prob (F-statistic):</th> <td>1.12e-24</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:45:52</td>     <th>  Log-Likelihood:    </th> <td> -18491.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>3.699e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1458</td>      <th>  BIC:               </th> <td>3.700e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>   <td> 1.588e+05</td> <td> 2914.717</td> <td>   54.495</td> <td> 0.000</td> <td> 1.53e+05</td> <td> 1.65e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LotArea</th> <td>    2.1000</td> <td>    0.201</td> <td>   10.445</td> <td> 0.000</td> <td>    1.706</td> <td>    2.494</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>587.660</td> <th>  Durbin-Watson:     </th> <td>   1.998</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3374.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.788</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 9.532</td>  <th>  Cond. No.          </th> <td>2.11e+04</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.11e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    SalePrice     & \\textbf{  R-squared:         } &     0.070   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.069   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     109.1   \\\\\n",
       "\\textbf{Date:}             & Mon, 25 Dec 2023 & \\textbf{  Prob (F-statistic):} &  1.12e-24   \\\\\n",
       "\\textbf{Time:}             &     15:45:52     & \\textbf{  Log-Likelihood:    } &   -18491.   \\\\\n",
       "\\textbf{No. Observations:} &        1460      & \\textbf{  AIC:               } & 3.699e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &        1458      & \\textbf{  BIC:               } & 3.700e+04   \\\\\n",
       "\\textbf{Df Model:}         &           1      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                 & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}   &    1.588e+05  &     2914.717     &    54.495  &         0.000        &     1.53e+05    &     1.65e+05     \\\\\n",
       "\\textbf{LotArea} &       2.1000  &        0.201     &    10.445  &         0.000        &        1.706    &        2.494     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 587.660 & \\textbf{  Durbin-Watson:     } &    1.998  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3374.003  \\\\\n",
       "\\textbf{Skew:}          &   1.788 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &   9.532 & \\textbf{  Cond. No.          } & 2.11e+04  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.11e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              SalePrice   R-squared:                       0.070\n",
       "Model:                            OLS   Adj. R-squared:                  0.069\n",
       "Method:                 Least Squares   F-statistic:                     109.1\n",
       "Date:                Mon, 25 Dec 2023   Prob (F-statistic):           1.12e-24\n",
       "Time:                        15:45:52   Log-Likelihood:                -18491.\n",
       "No. Observations:                1460   AIC:                         3.699e+04\n",
       "Df Residuals:                    1458   BIC:                         3.700e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       1.588e+05   2914.717     54.495      0.000    1.53e+05    1.65e+05\n",
       "LotArea        2.1000      0.201     10.445      0.000       1.706       2.494\n",
       "==============================================================================\n",
       "Omnibus:                      587.660   Durbin-Watson:                   1.998\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3374.003\n",
       "Skew:                           1.788   Prob(JB):                         0.00\n",
       "Kurtosis:                       9.532   Cond. No.                     2.11e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.11e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmvar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#other way using sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = hp['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvar = hp[['LotArea']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmvar2 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmvar2.fit(Xvar,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158836.1518968766 [2.09997195]\n"
     ]
    }
   ],
   "source": [
    "print(lmvar2.intercept_, lmvar2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LinearRegression in module sklearn.linear_model._base object:\n",
      "\n",
      "class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      " |  LinearRegression(*, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
      " |  \n",
      " |  Ordinary least squares Linear Regression.\n",
      " |  \n",
      " |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      " |  to minimize the residual sum of squares between the observed targets in\n",
      " |  the dataset, and the targets predicted by the linear approximation.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to False, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to use for the computation. This will only provide\n",
      " |      speedup in case of sufficiently large problems, that is if firstly\n",
      " |      `n_targets > 1` and secondly `X` is sparse or if `positive` is set\n",
      " |      to `True`. ``None`` means 1 unless in a\n",
      " |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      " |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  positive : bool, default=False\n",
      " |      When set to ``True``, forces the coefficients to be positive. This\n",
      " |      option is only supported for dense arrays.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      " |      Estimated coefficients for the linear regression problem.\n",
      " |      If multiple targets are passed during the fit (y 2D), this\n",
      " |      is a 2D array of shape (n_targets, n_features), while if only\n",
      " |      one target is passed, this is a 1D array of length n_features.\n",
      " |  \n",
      " |  rank_ : int\n",
      " |      Rank of matrix `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  singular_ : array of shape (min(X, y),)\n",
      " |      Singular values of `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  intercept_ : float or array of shape (n_targets,)\n",
      " |      Independent term in the linear model. Set to 0.0 if\n",
      " |      `fit_intercept = False`.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  Ridge : Ridge regression addresses some of the\n",
      " |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      " |      size of the coefficients with l2 regularization.\n",
      " |  Lasso : The Lasso is a linear model that estimates\n",
      " |      sparse coefficients with l1 regularization.\n",
      " |  ElasticNet : Elastic-Net is a linear regression\n",
      " |      model trained with both l1 and l2 -norm regularization of the\n",
      " |      coefficients.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  From the implementation point of view, this is just plain Ordinary\n",
      " |  Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
      " |  (scipy.optimize.nnls) wrapped as a predictor object.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import LinearRegression\n",
      " |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      " |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      " |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      " |  >>> reg = LinearRegression().fit(X, y)\n",
      " |  >>> reg.score(X, y)\n",
      " |  1.0\n",
      " |  >>> reg.coef_\n",
      " |  array([1., 2.])\n",
      " |  >>> reg.intercept_\n",
      " |  3.0...\n",
      " |  >>> reg.predict(np.array([[3, 5]]))\n",
      " |  array([16.])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.utils._metadata_requests._MetadataRequester\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, fit_intercept=True, copy_X=True, n_jobs=None, positive=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values. Will be cast to X's dtype if necessary.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             parameter *sample_weight* support to LinearRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted Estimator.\n",
      " |  \n",
      " |  set_fit_request(self: sklearn.linear_model._base.LinearRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._base.LinearRegression\n",
      " |      Request metadata passed to the ``fit`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  set_score_request(self: sklearn.linear_model._base.LinearRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._base.LinearRegression\n",
      " |      Request metadata passed to the ``score`` method.\n",
      " |      \n",
      " |      Note that this method is only relevant if\n",
      " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
      " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      The options for each parameter are:\n",
      " |      \n",
      " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
      " |      \n",
      " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
      " |      \n",
      " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
      " |      \n",
      " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
      " |      \n",
      " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
      " |      existing request. This allows you to change the request for some\n",
      " |      parameters and not others.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is only relevant if this estimator is used as a\n",
      " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
      " |          :class:`pipeline.Pipeline`. Otherwise it has no effect.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
      " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          The updated object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __sklearn_clone__(self)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  get_metadata_routing(self)\n",
      " |      Get metadata routing of this object.\n",
      " |      \n",
      " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
      " |      mechanism works.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      routing : MetadataRequest\n",
      " |          A :class:`~utils.metadata_routing.MetadataRequest` encapsulating\n",
      " |          routing information.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from abc.ABCMeta\n",
      " |      Set the ``set_{method}_request`` methods.\n",
      " |      \n",
      " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
      " |      looks for the information available in the set default values which are\n",
      " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
      " |      from method signatures.\n",
      " |      \n",
      " |      The ``__metadata_request__*`` class attributes are used when a method\n",
      " |      does not explicitly accept a metadata through its arguments or if the\n",
      " |      developer would like to specify a request value for those metadata\n",
      " |      which are different from the default ``None``.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lmvar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176580.91488881, 178995.88263327, 182460.83635359, ...,\n",
       "       177824.09828422, 179241.57935162, 179703.573181  ])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmvar2.predict(Xvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.JointGrid at 0x24634bb5810>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.jointplot(x=hp['LotArea'], y = hp['SalePrice'], data = hp, kind = 'reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple Linear Regression -  the primary goal in Multiple Linear Regression. The coefficients are estimated to minimize the sum of squared differences between observed and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpm = pd.read_csv('house price.csv',header=0, index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpm = hpm[['SalePrice', 'LotArea', 'OverallCond', 'OverallQual']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>OverallQual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>208500</td>\n",
       "      <td>8450</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181500</td>\n",
       "      <td>9600</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>223500</td>\n",
       "      <td>11250</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140000</td>\n",
       "      <td>9550</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250000</td>\n",
       "      <td>14260</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>175000</td>\n",
       "      <td>7917</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>210000</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>266500</td>\n",
       "      <td>9042</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>142125</td>\n",
       "      <td>9717</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>147500</td>\n",
       "      <td>9937</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SalePrice  LotArea  OverallCond  OverallQual\n",
       "Id                                                \n",
       "1        208500     8450            5            7\n",
       "2        181500     9600            8            6\n",
       "3        223500    11250            5            7\n",
       "4        140000     9550            5            7\n",
       "5        250000    14260            5            8\n",
       "...         ...      ...          ...          ...\n",
       "1456     175000     7917            5            6\n",
       "1457     210000    13175            6            6\n",
       "1458     266500     9042            9            7\n",
       "1459     142125     9717            6            5\n",
       "1460     147500     9937            6            5\n",
       "\n",
       "[1460 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multivar = hpm.drop('SalePrice',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>OverallQual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8450</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9600</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11250</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9550</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14260</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>7917</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>9042</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>9717</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>9937</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LotArea  OverallCond  OverallQual\n",
       "Id                                     \n",
       "1        8450            5            7\n",
       "2        9600            8            6\n",
       "3       11250            5            7\n",
       "4        9550            5            7\n",
       "5       14260            5            8\n",
       "...       ...          ...          ...\n",
       "1456     7917            5            6\n",
       "1457    13175            6            6\n",
       "1458     9042            9            7\n",
       "1459     9717            6            5\n",
       "1460     9937            6            5\n",
       "\n",
       "[1460 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_multivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_multivar = hpm['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "1    208500\n",
       "2    181500\n",
       "3    223500\n",
       "4    140000\n",
       "5    250000\n",
       "Name: SalePrice, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_multivar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multivar_cons = sn.add_constant(X_multivar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>OverallQual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>1.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      const  LotArea  OverallCond  OverallQual\n",
       "Id                                            \n",
       "1       1.0     8450            5            7\n",
       "2       1.0     9600            8            6\n",
       "3       1.0    11250            5            7\n",
       "4       1.0     9550            5            7\n",
       "5       1.0    14260            5            8\n",
       "...     ...      ...          ...          ...\n",
       "1456    1.0     7917            5            6\n",
       "1457    1.0    13175            6            6\n",
       "1458    1.0     9042            9            7\n",
       "1459    1.0     9717            6            5\n",
       "1460    1.0     9937            6            5\n",
       "\n",
       "[1460 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_multivar_cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_multivar = sn.OLS(y_multivar, X_multivar_cons).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>SalePrice</td>    <th>  R-squared:         </th> <td>   0.659</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.658</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   935.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 25 Dec 2023</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>16:23:44</td>     <th>  Log-Likelihood:    </th> <td> -17760.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>3.553e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1456</td>      <th>  BIC:               </th> <td>3.555e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>-1.022e+05</td> <td> 8633.073</td> <td>  -11.832</td> <td> 0.000</td> <td>-1.19e+05</td> <td>-8.52e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LotArea</th>     <td>    1.4503</td> <td>    0.123</td> <td>   11.831</td> <td> 0.000</td> <td>    1.210</td> <td>    1.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallCond</th> <td> -423.6737</td> <td> 1097.973</td> <td>   -0.386</td> <td> 0.700</td> <td>-2577.451</td> <td> 1730.104</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallQual</th> <td>  4.43e+04</td> <td>  888.434</td> <td>   49.860</td> <td> 0.000</td> <td> 4.26e+04</td> <td>  4.6e+04</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>574.483</td> <th>  Durbin-Watson:     </th> <td>   1.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>7330.883</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.468</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>13.578</td>  <th>  Cond. No.          </th> <td>1.04e+05</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.04e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &    SalePrice     & \\textbf{  R-squared:         } &     0.659   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.658   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     935.9   \\\\\n",
       "\\textbf{Date:}             & Mon, 25 Dec 2023 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}             &     16:23:44     & \\textbf{  Log-Likelihood:    } &   -17760.   \\\\\n",
       "\\textbf{No. Observations:} &        1460      & \\textbf{  AIC:               } & 3.553e+04   \\\\\n",
       "\\textbf{Df Residuals:}     &        1456      & \\textbf{  BIC:               } & 3.555e+04   \\\\\n",
       "\\textbf{Df Model:}         &           3      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                     & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}       &   -1.022e+05  &     8633.073     &   -11.832  &         0.000        &    -1.19e+05    &    -8.52e+04     \\\\\n",
       "\\textbf{LotArea}     &       1.4503  &        0.123     &    11.831  &         0.000        &        1.210    &        1.691     \\\\\n",
       "\\textbf{OverallCond} &    -423.6737  &     1097.973     &    -0.386  &         0.700        &    -2577.451    &     1730.104     \\\\\n",
       "\\textbf{OverallQual} &     4.43e+04  &      888.434     &    49.860  &         0.000        &     4.26e+04    &      4.6e+04     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 574.483 & \\textbf{  Durbin-Watson:     } &    1.966  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 7330.883  \\\\\n",
       "\\textbf{Skew:}          &   1.468 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  13.578 & \\textbf{  Cond. No.          } & 1.04e+05  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.04e+05. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              SalePrice   R-squared:                       0.659\n",
       "Model:                            OLS   Adj. R-squared:                  0.658\n",
       "Method:                 Least Squares   F-statistic:                     935.9\n",
       "Date:                Mon, 25 Dec 2023   Prob (F-statistic):               0.00\n",
       "Time:                        16:23:44   Log-Likelihood:                -17760.\n",
       "No. Observations:                1460   AIC:                         3.553e+04\n",
       "Df Residuals:                    1456   BIC:                         3.555e+04\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const       -1.022e+05   8633.073    -11.832      0.000   -1.19e+05   -8.52e+04\n",
       "LotArea         1.4503      0.123     11.831      0.000       1.210       1.691\n",
       "OverallCond  -423.6737   1097.973     -0.386      0.700   -2577.451    1730.104\n",
       "OverallQual   4.43e+04    888.434     49.860      0.000    4.26e+04     4.6e+04\n",
       "==============================================================================\n",
       "Omnibus:                      574.483   Durbin-Watson:                   1.966\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             7330.883\n",
       "Skew:                           1.468   Prob(JB):                         0.00\n",
       "Kurtosis:                      13.578   Cond. No.                     1.04e+05\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.04e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_multivar.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmvar3 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lmvar3.fit(X_multivar, y_multivar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-102150.53018167955 [ 1.45029563e+00 -4.23673738e+02  4.42969972e+04]\n"
     ]
    }
   ],
   "source": [
    "print(lmvar3.intercept_,lmvar3.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-Statistics\n",
    "- If we depend on individual p values, there is a very high chance that we will incorrectly conclude that there is a relationship between predictor variables and the response variables. The solution to this problem is to adjust the number of predictors and then find the p value with new statistics, which adjusts the number of predictors and this is called as F-statistics.\n",
    "- If the p value is lower than the threshold value, say 1% of 5%, again, we said that the model predictors are significantly impacting the response. \n",
    "-  check whether your p value of F-statistics is lower than the threshold of 5%. This will ensure that the model predictors that are used are having some significant impact on the response variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOr results stats of categorical Variables \n",
    "- you're having two possibilities, one is if there is an airport, which means the value of xi is one, then we'll have a value of other operators like x and coefficients + constant + beta 1. Else if it is 0, then beta 1 will not be present, xi is 0, so beta 1 into xi is 0. Therefore, this beta of airport is straightaway, giving us a difference in house price if there is an airport and if there is not an airport. So if I look at the result of my model, like airportYes variable has an estimate of somewhere around 1.13. Now, this is the beta value, so this means that if there is an airport, all the other variables are same and all the value of the house of the price will increase by 1.13 unit. Also let's have a look at the p-value. Now, p-value is low, which means there is a statistical evidence of the difference in house price depending on whether there is an airport or whether there is not an airport.\n",
    "- this is how your qualitative variables are handled and interpreted in linear model. We will first transform them to dummy variables of n- 1 categories. Then we will run a regression, then looking at the betas and the p values we can interpret the result like this\n",
    "- In logistic regression, one category is chosen as the reference, and the coefficients for other categories are compared to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_multivar, y_multivar, test_size = 0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 3) (292, 3) (1168,) (292,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_a = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_a.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_a = lm_a.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_a = lm_a.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6474852261082007"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6612686856686455"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_train, y_train_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r^2 value is of more importance as compared to the training set. We should always look at our test score instead of training r^2values. To evaluate the performance of our model, this is how you can split your data into test and train, and then evaluate at proper perceptions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
